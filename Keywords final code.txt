from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

stop=set(stopwords.words('english'))
stri=""

import os
  
# Folder Path
path = "C:\\Users\\rahul\\OneDrive\\Desktop\\Web Scraper\\Database"
  
# Change the directory
os.chdir(path)

key =list()
# iterate through all file
for file in os.listdir():
    # Check whether file is in text format or not
    if file.endswith(".txt"):
        file_path = f"{path}\{file}"
        with open(file_path) as f:
            data =f.readlines()
            for x in data:
                val=x.strip()
                stri=stri+val
            tok=word_tokenize(stri)
            ans=list()
            for i in range(0,len(tok)):
                word=tok[i].lower()
                if word in stop:
                    pass
                elif word.isalpha():
                    ans.append(word)
                else:
                    pass
            none=""
            ans = list(map(lambda x: x.replace('\n',none).replace('\\n',none).replace(',',none),ans))
            for i in range(0,len(ans)):
                ans[i]=ans[i].lower()
                if ans[i]=='' or ans[i]=='.':
                    pass
                else:
                    key.append(ans[i])
            ans.clear()
            stri=""

key_final=list()

for i in key:
    if i in key_final:
        pass
    else:
        key_final.append(i)

key_final.sort()
#print(key_final)
with open("keywords.txt","w+") as f:
    f.write('\n'.join(key_final))
